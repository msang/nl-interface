{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5768d93-cd20-4d29-ad8e-08f9fcd64236",
   "metadata": {},
   "source": [
    "# Computing Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72ce2a-a564-4f1d-9478-5919c7387e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install krippendorff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac72904-6b11-41d4-bc45-2d4da645c037",
   "metadata": {},
   "source": [
    "Import all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d610d3a0-d891-490f-9067-648882c1c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excl = ['id','intent','text','prompts','output']\n",
    "\n",
    "crb_df = pd.read_csv(\"EnergyFeedbackGeneration-EvaluationResults-Cerbero-7B.tsv\", sep=\"\\t\").drop(excl, axis=1)\n",
    "l2_df = pd.read_csv(\"EnergyFeedbackGeneration-EvaluationResults-LLaMAntino2.tsv\", sep=\"\\t\").drop(excl, axis=1)\n",
    "l3_df = pd.read_csv(\"EnergyFeedbackGeneration-EvaluationResults-LLaMAntino3.tsv\", sep=\"\\t\").drop(excl, axis=1)\n",
    "zef_df = pd.read_csv(\"EnergyFeedbackGeneration-EvaluationResults-Zefiro.tsv\", sep=\"\\t\").drop(excl, axis=1)\n",
    "#l2_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b25a79-4dda-40a2-b039-9ccce5c4eed4",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0e3fb7c9-6f4e-4658-8d7b-6e11339b3d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerbero\n",
      "Krippendorff's alpha for Usefulness: 0.40\n",
      "Krippendorff's alpha for Necessity: 0.34\n",
      "Krippendorff's alpha for Understandability: 0.24\n",
      "Krippendorff's alpha for Fluency: 0.31\n",
      "Krippendorff's alpha for Accuracy: 0.47\n",
      "LLaMAntino2\n",
      "Krippendorff's alpha for Usefulness: 0.78\n",
      "Krippendorff's alpha for Necessity: 0.78\n",
      "Krippendorff's alpha for Understandability: 0.34\n",
      "Krippendorff's alpha for Fluency: -0.01\n",
      "Krippendorff's alpha for Accuracy: 0.80\n",
      "LLaMAntino3\n",
      "Krippendorff's alpha for Usefulness: 0.30\n",
      "Krippendorff's alpha for Necessity: 0.35\n",
      "Krippendorff's alpha for Understandability: 0.13\n",
      "Krippendorff's alpha for Fluency: 0.26\n",
      "Krippendorff's alpha for Accuracy: 0.68\n",
      "Zefiro\n",
      "Krippendorff's alpha for Usefulness: 0.35\n",
      "Krippendorff's alpha for Necessity: 0.37\n",
      "Krippendorff's alpha for Understandability: 0.13\n",
      "Krippendorff's alpha for Fluency: 0.23\n",
      "Krippendorff's alpha for Accuracy: 0.60\n",
      "Average alpha for Usefulness: 0.46\n",
      "Average alpha for Necessity: 0.46\n",
      "Average alpha for Understandability: 0.21\n",
      "Average alpha for Fluency: 0.20\n",
      "Average alpha for Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "import krippendorff\n",
    "\n",
    "criteria = ['Usefulness', 'Necessity', 'Understandability', 'Fluency', 'Accuracy']\n",
    "model_names=['Cerbero', 'LLaMAntino2', 'LLaMAntino3', 'Zefiro']\n",
    "model_dfs = [crb_df, l2_df, l3_df, zef_df]\n",
    "\n",
    "def compute_alpha(df, criteria):\n",
    "\n",
    "    iaa = {}\n",
    "\n",
    "    for criterion in criteria:\n",
    "        columns = [col for col in df.columns if col.startswith(criterion)]\n",
    "        data = df[columns].values.T\n",
    "        #print(data)\n",
    "        alpha = krippendorff.alpha(reliability_data=data)\n",
    "        iaa[criterion] = alpha\n",
    "        print(f\"Krippendorff's alpha for {criterion}: {alpha:.2f}\")\n",
    "\n",
    "    return iaa\n",
    "\n",
    "\n",
    "def compute_average_alpha(criteria, model_iaas):\n",
    "    average_iaa = {}\n",
    "    for criterion in criteria:\n",
    "        alphas = [model_iaa[criterion] for model_iaa in model_iaas]\n",
    "        average_alpha = sum(alphas) / len(alphas)\n",
    "        average_iaa[criterion] = average_alpha\n",
    "        print(f\"Average alpha for {criterion}: {average_alpha:.2f}\")\n",
    "    return average_iaa\n",
    "\n",
    "# collect IAAs for each model\n",
    "model_iaas = []\n",
    "for model_name, df in zip(model_names, model_dfs):\n",
    "    print(model_name)\n",
    "    iaa = compute_alpha(df, criteria)\n",
    "    model_iaas.append(iaa)\n",
    "\n",
    "average_iaa = compute_average_alpha(criteria, model_iaas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf1623-fa16-437f-a3e6-0ddf7d77c054",
   "metadata": {},
   "source": [
    "Compute pairwise percentage agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "abaedf95-a1e9-4791-b426-c2d38ce29398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerbero\n",
      "{'Usefulness': 0.4166666666666667, 'Necessity': 0.5233333333333333, 'Understandability': 0.5166666666666667, 'Fluency': 0.4533333333333333, 'Accuracy': 0.66}\n",
      "LLaMAntino2\n",
      "{'Usefulness': 0.39666666666666667, 'Necessity': 0.44666666666666666, 'Understandability': 0.37333333333333335, 'Fluency': 0.31666666666666665, 'Accuracy': 0.5566666666666666}\n",
      "LLaMAntino3\n",
      "{'Usefulness': 0.3933333333333333, 'Necessity': 0.31333333333333335, 'Understandability': 0.5033333333333333, 'Fluency': 0.4533333333333333, 'Accuracy': 0.6466666666666666}\n",
      "Zefiro\n",
      "{'Usefulness': 0.37, 'Necessity': 0.41, 'Understandability': 0.5533333333333333, 'Fluency': 0.43666666666666665, 'Accuracy': 0.58}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Usefulness': 0.39416666666666667,\n",
       " 'Necessity': 0.4233333333333333,\n",
       " 'Understandability': 0.4866666666666667,\n",
       " 'Fluency': 0.41500000000000004,\n",
       " 'Accuracy': 0.6108333333333333}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_percent_agreement(df, criteria):\n",
    "    iaa = {}\n",
    "    for criterion in criteria:\n",
    "        columns = [col for col in df.columns if col.startswith(criterion)]\n",
    "        data = df[columns].values.T\n",
    "        #print(data)\n",
    "        num_items = data.shape[1]     \n",
    "        num_annotators = data.shape[0]\n",
    "        agreements = 0\n",
    "        tot_comparisons=0\n",
    "\n",
    "        # compare each pair of annotations\n",
    "        for i in range(num_annotators):\n",
    "            for j in range(i+1, num_annotators):\n",
    "                #print([i, j])\n",
    "                agreements += np.sum(data[i] == data[j])\n",
    "                tot_comparisons += 1\n",
    "                #print(tot_comparisons)\n",
    "                \n",
    "        iaa[criterion] = agreements / (num_items*tot_comparisons)      \n",
    "\n",
    "    return iaa\n",
    "\n",
    "\n",
    "def compute_average_percent(criteria, model_iaas):\n",
    "    average_iaa = {}\n",
    "    for criterion in criteria:\n",
    "        percent = [model_iaa[criterion] for model_iaa in model_iaas]\n",
    "        average_percent = sum(percent) / len(percent)\n",
    "        average_iaa[criterion] = average_percent\n",
    "        #print(f\"Average percentage agreement for {criterion}: {average_percent:.2f}\")\n",
    "    return average_iaa\n",
    "\n",
    "# Percentage agreement for the given annotations\n",
    "model_iaas = []\n",
    "for model_name, df in zip(model_names, model_dfs):\n",
    "    print(model_name)\n",
    "    iaa = compute_percent_agreement(df, criteria)\n",
    "    print(iaa)\n",
    "    model_iaas.append(iaa)\n",
    "\n",
    "compute_average_percent(criteria, model_iaas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51d3b4-4532-4a78-a349-b9d6bff0b6f1",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03438042-471f-4fff-bce7-7f2c4f4a1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9384d61d-834c-4fe5-acd6-81f53d7b40ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=84.26518476097468, pvalue=3.7314438905209685e-18)\n",
      "                  Cerbero   LLaMAntino2   LLaMAntino3        Zefiro\n",
      "Cerbero      1.000000e+00  5.266857e-10  8.601793e-01  1.000000e+00\n",
      "LLaMAntino2  5.266857e-10  1.000000e+00  1.119024e-14  7.178825e-14\n",
      "LLaMAntino3  8.601793e-01  1.119024e-14  1.000000e+00  1.000000e+00\n",
      "Zefiro       1.000000e+00  7.178825e-14  1.000000e+00  1.000000e+00\n",
      "KruskalResult(statistic=84.98586353457488, pvalue=2.6133076249360153e-18)\n",
      "                  Cerbero   LLaMAntino2  LLaMAntino3        Zefiro\n",
      "Cerbero      1.000000e+00  3.175125e-18     0.000613  9.978984e-02\n",
      "LLaMAntino2  3.175125e-18  1.000000e+00     0.000003  4.472457e-10\n",
      "LLaMAntino3  6.132525e-04  3.089213e-06     1.000000  8.164995e-01\n",
      "Zefiro       9.978984e-02  4.472457e-10     0.816500  1.000000e+00\n",
      "KruskalResult(statistic=68.62403126742905, pvalue=8.411140077181468e-15)\n",
      "                  Cerbero   LLaMAntino2   LLaMAntino3        Zefiro\n",
      "Cerbero      1.000000e+00  2.544223e-07  1.000000e+00  1.647272e-01\n",
      "LLaMAntino2  2.544223e-07  1.000000e+00  1.025729e-09  9.141847e-14\n",
      "LLaMAntino3  1.000000e+00  1.025729e-09  1.000000e+00  1.000000e+00\n",
      "Zefiro       1.647272e-01  9.141847e-14  1.000000e+00  1.000000e+00\n",
      "KruskalResult(statistic=45.30579066728179, pvalue=7.966662178294436e-10)\n",
      "              Cerbero   LLaMAntino2   LLaMAntino3        Zefiro\n",
      "Cerbero      1.000000  1.593382e-04  5.696890e-01  7.058736e-01\n",
      "LLaMAntino2  0.000159  1.000000e+00  2.598990e-08  4.873920e-08\n",
      "LLaMAntino3  0.569689  2.598990e-08  1.000000e+00  1.000000e+00\n",
      "Zefiro       0.705874  4.873920e-08  1.000000e+00  1.000000e+00\n",
      "KruskalResult(statistic=120.56930420407616, pvalue=5.818686270790282e-26)\n",
      "                  Cerbero   LLaMAntino2   LLaMAntino3        Zefiro\n",
      "Cerbero      1.000000e+00  4.579845e-22  1.000000e+00  6.701189e-01\n",
      "LLaMAntino2  4.579845e-22  1.000000e+00  1.391656e-16  9.621996e-16\n",
      "LLaMAntino3  1.000000e+00  1.391656e-16  1.000000e+00  1.000000e+00\n",
      "Zefiro       6.701189e-01  9.621996e-16  1.000000e+00  1.000000e+00\n",
      "Eta-squared for Usefulness: η² = 0.1055\n",
      "Eta-squared for Necessity: η² = 0.1064\n",
      "Eta-squared for Understandability: η² = 0.0859\n",
      "Eta-squared for Fluency: η² = 0.0567\n",
      "Eta-squared for Accuracy: η² = 0.1509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# conflate annotations keeping track of annotators\n",
    "def conflate_annotations(df, criteria):\n",
    "    conflated_df = pd.DataFrame()\n",
    "    for criterion in criteria:\n",
    "        criterion_scores = []\n",
    "        annotators = []\n",
    "        for i in range(4):\n",
    "            column = f\"{criterion}_A{i+1}\"\n",
    "            scores = df[column].values\n",
    "            criterion_scores.extend(scores)\n",
    "            annotator_info = [f\"A{i+1}\"] * len(scores)\n",
    "            annotators.extend(annotator_info)\n",
    "        conflated_df[criterion] = criterion_scores\n",
    "        conflated_df[f\"{criterion}_Annotator\"] = annotators\n",
    "    return conflated_df\n",
    "\n",
    "conflated_dfs = [conflate_annotations(df, criteria) for df in model_dfs]\n",
    "\n",
    "# combine all models into a single dataframe for each criterion\n",
    "data = []\n",
    "for model_name, conflated_df in zip(model_names, conflated_dfs):\n",
    "    for criterion in criteria:\n",
    "        for idx, score in enumerate(conflated_df[criterion].values):\n",
    "            annotator = conflated_df[f\"{criterion}_Annotator\"].values[idx]\n",
    "            data.append({'Model': model_name, 'Criterion': criterion, 'Score': score, 'Annotator': annotator})\n",
    "\n",
    "# df with ratings for each criterion and model\n",
    "df = pd.DataFrame(data)\n",
    "#print(df.head(10))\n",
    "\n",
    "def eta_squared(chi2, N):\n",
    "    return chi2 / (N - 1)\n",
    "\n",
    "# perform Kruskal-Wallis test + η² calculation + Dunn's test for each criterion \n",
    "eta_squared_results = {}\n",
    "for criterion in df['Criterion'].unique():\n",
    "    \n",
    "    print(criterion)\n",
    "    df_criterion = df[df['Criterion'] == criterion]\n",
    "    \n",
    "    # remove NaN values, if present\n",
    "    groups = [df_criterion[df_criterion['Model'] == model]['Score'].dropna().values for model in df_criterion['Model'].unique()]\n",
    "    \n",
    "    #print(f\"Groups for {criterion}:\")\n",
    "    #for i, group in enumerate(groups):\n",
    "        #print(f\"Group {i+1}: {group}\")\n",
    "\n",
    "    # Check for identical values or insufficient variability\n",
    "    #if any(len(set(group)) == 1 for group in groups) or len(groups) <= 1:\n",
    "    #    print(f\"Skipping Kruskal-Wallis test for {criterion} due to insufficient variability.\")\n",
    "    #    eta_squared_results[criterion] = float('nan')\n",
    "    #    continue\n",
    "\n",
    "    kruskal_test = kruskal(*groups)\n",
    "    print(kruskal_test)\n",
    "    \n",
    "    chi_square = kruskal_test.statistic\n",
    "    N = len(df_criterion.dropna())\n",
    "    eta_sq = eta_squared(chi_square, N)   \n",
    "    eta_squared_results[criterion] = eta_sq\n",
    "\n",
    "    # Dunn's test with Bonferroni correction\n",
    "    dunn_results = sp.posthoc_dunn(df_criterion, val_col='Score', group_col='Model', p_adjust='bonferroni')\n",
    "    print(dunn_results)\n",
    "\n",
    "\n",
    "for criterion, eta_sq in eta_squared_results.items():\n",
    "    print(f'Eta-squared for {criterion}: η² = {eta_sq:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8384c1-9f6b-4acb-b8a9-2ca94f505e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
